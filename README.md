# ğŸ›’ Pipeline de Dados E-commerce Brasil

Pipeline moderno para ingestÃ£o e anÃ¡lise de pedidos de e-commerce no Brasil.  
O fluxo integra **Kafka**, **Spark** e **PostgreSQL** para transformar dados brutos de vendas em **insights acionÃ¡veis**.

---

## ğŸ” VisÃ£o Geral

O objetivo Ã© automatizar a jornada dos dados:

1. **Coleta** â€” pedidos extraÃ­dos da API da loja (ou dataset sintÃ©tico incluÃ­do).  
2. **IngestÃ£o** â€” dados enviados para o **Kafka**.  
3. **Processamento** â€” **Spark** organiza, enriquece e valida as mensagens.  
4. **Armazenamento** â€” registros consolidados no **PostgreSQL**.  
5. **AnÃ¡lise** â€” consultas SQL ou ferramentas de BI exploram os dados tratados.

```
Loja/API â†’ Kafka â†’ Spark â†’ PostgreSQL â†’ BI/RelatÃ³rios
```

---

## ğŸ“‚ Estrutura do Projeto
```
â”œâ”€â”€ data/                 
â”‚   â””â”€â”€ pedidos.csv            # Dados sintÃ©ticos de exemplo (50k linhas)
â”œâ”€â”€ src/                      
â”‚   â”œâ”€â”€ extract_orders.py      # Exemplo de extraÃ§Ã£o (mock/API)
â”‚   â”œâ”€â”€ spark_kafka_producer.py# Envia pedidos CSV para Kafka via Spark
â”‚   â”œâ”€â”€ kafka_consumer.py      # Consome Kafka e grava no PostgreSQL
â”‚   â”œâ”€â”€ postgres_query_tool.py # Consultas prÃ©-definidas no PostgreSQL
â”‚   â”œâ”€â”€ config.py              # VariÃ¡veis de conexÃ£o (Kafka, Postgres, API)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py          # Logger padronizado
â”œâ”€â”€ docker-compose.yaml        # Infra: Kafka, Zookeeper e Postgres
â”œâ”€â”€ requirements.txt           # DependÃªncias Python
â””â”€â”€ README.md
```

---

## âš™ï¸ PrÃ©-requisitos

- Python **3.8+**  
- Docker + Docker Compose  
- Spark instalado localmente  
- Cliente PostgreSQL (ou DBeaver, pgAdmin)  

---

## ğŸš€ Como Executar

### 1) Clonar e instalar dependÃªncias
```bash
git clone <url-do-repo>
cd pipeline_ecommerce
pip install -r requirements.txt
```

### 2) Subir Kafka e Postgres
```bash
docker-compose up -d
```
> Postgres padrÃ£o: `postgresql://app:app@localhost:5432/ecommerce`

### 3) Produzir mensagens no Kafka
```bash
spark-submit   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1   src/spark_kafka_producer.py   --csv data/pedidos.csv --topic pedidos_ecommerce
```

### 4) Consumir e gravar no Postgres
```bash
python src/kafka_consumer.py --topic pedidos_ecommerce --group ecommerce-consumers
```

### 5) Consultar os dados
```bash
# Top 10 produtos
python src/postgres_query_tool.py top-products --limit 10 --out top10_produtos.csv

# Top 10 cidades
python src/postgres_query_tool.py top-cities --limit 10 --out top10_cidades.csv
```

---

## ğŸ“Š Exemplos de AnÃ¡lises

- Produtos mais vendidos por regiÃ£o.  
- Receita acumulada por categoria.  
- Desempenho de canais de venda (Site, App, Marketplace).  
- Status dos pedidos (Pago, Pendente, Cancelado, Reembolsado).  

---

## ğŸ› ï¸ Tecnologias

- **Python** â€“ Scripts de extraÃ§Ã£o, consumo e consultas.  
- **Apache Spark** â€“ Processamento em lote e integraÃ§Ã£o com Kafka.  
- **Apache Kafka** â€“ Streaming de dados de pedidos.  
- **PostgreSQL** â€“ Armazenamento analÃ­tico e consultas.  
- **Docker Compose** â€“ OrquestraÃ§Ã£o de Kafka, Zookeeper e Postgres.  

---

## ğŸ“Œ Roadmap

- [ ] Implementar ingestÃ£o contÃ­nua com Spark Structured Streaming.  
- [ ] Criar dashboards em Power BI / Superset.  
- [ ] Adicionar camada de qualidade dos dados (validaÃ§Ã£o e alerta).  
- [ ] Automatizar pipeline no Airflow.  

---

âœ¨ Desenvolvido como projeto educacional para **engenharia de dados aplicada a e-commerce**.  
